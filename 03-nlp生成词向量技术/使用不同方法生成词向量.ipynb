{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ff1bb26",
   "metadata": {},
   "source": [
    "# one-hot表示词向量（基于numpy实现）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "622d80d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\priv_\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.436 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我 爱 我 的 祖国', '我 喜欢 祖国 的 大好河山']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "data = [\"我爱我的祖国\",'我喜欢祖国的大好河山']\n",
    "samples = [' '.join(jieba.cut(doc)) for doc in data]\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d8bf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'我': 0, '爱': 1, '的': 2, '祖国': 3, '喜欢': 4, '大好河山': 5}\n",
      "[[[1. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 词级别的one-hot编码\n",
    "import numpy as np\n",
    "\n",
    "token_index = {}  # 构造一个空的索引集合\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index)  # 给每个唯一单词指定一个唯一索引\n",
    "max_length = len(token_index)  # 对样本进行分词，只考虑每个样本前max-length个单词\n",
    "results = np.zeros(shape=(len(samples), max_length, max(token_index.values())+1))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1\n",
    "print(token_index)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf457152",
   "metadata": {},
   "source": [
    "# Bag of words表示词向量（基于sklearn的实现）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f09b722a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我 爱 我 的 祖国', '我 喜欢 祖国 的 大好河山']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc56d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['喜欢', '大好河山', '我', '爱', '的', '祖国']\n",
      "[[0 0 2 1 1 1]\n",
      " [1 1 1 0 1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\priv_\\.conda\\envs\\py37_torch18_cuda111\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b', binary=False)\n",
    "X = vectorizer.fit_transform(samples)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f68172",
   "metadata": {},
   "source": [
    "# TF_IDF表示词向量（基于sklearn实现）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f90009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['喜欢', '大好河山', '我', '爱', '的', '祖国']\n",
      "  (0, 5)\t0.3540997415957358\n",
      "  (0, 4)\t0.3540997415957358\n",
      "  (0, 3)\t0.4976748316029239\n",
      "  (0, 2)\t0.7081994831914716\n",
      "  (1, 1)\t0.5330978245262535\n",
      "  (1, 0)\t0.5330978245262535\n",
      "  (1, 5)\t0.3793034928087496\n",
      "  (1, 4)\t0.3793034928087496\n",
      "  (1, 2)\t0.3793034928087496\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "X = vectorizer.fit_transform(samples)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fdb1f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.70819948 0.49767483 0.35409974 0.35409974]\n",
      " [0.53309782 0.53309782 0.37930349 0.         0.37930349 0.37930349]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ed4da",
   "metadata": {},
   "source": [
    "# 代码实现TF_IDF计算过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a668f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class TF_IDF_Model(object):\n",
    "    def __init__(self, documents_list):\n",
    "        # 文本列表\n",
    "        self.documents_list = documents_list\n",
    "        # 文本总个数\n",
    "        self.documents_number = len(documents_list)\n",
    "        # 存储每个文本中每个词的词频\n",
    "        self.tf = []\n",
    "        # 存储每个词汇的逆文档频率\n",
    "        self.idf = {}\n",
    "        # 类初始化\n",
    "        self.init()\n",
    "    \n",
    "    # 初始化，计算每个文本中的词频和逆文档频率\n",
    "    def init(self):\n",
    "        # 每个词汇的文档频率\n",
    "        df = {}\n",
    "        # 遍历每个文本列表中的每个文本\n",
    "        for document in self.documents_list:\n",
    "            temp = {}\n",
    "            # 遍历每个文本中的每个词\n",
    "            for word in document:\n",
    "                # 存储每个文本中每个词的词频\n",
    "                temp[word] = temp.get(word, 0) + 1/len(document)\n",
    "            # 遍历完一个文本，就将该文本计算的词频以字典的形式存储到tf列表中\n",
    "            self.tf.append(temp)\n",
    "            # 遍历temp中的词，每出现一次该词的文档频率就加一，计算该词的文本数\n",
    "            for key in temp.keys():\n",
    "                df[key] = df.get(key, 0) + 1\n",
    "        # 计算每个词的逆文档频率\n",
    "        for key, value in df.items():\n",
    "            self.idf[key] = np.log(self.documents_number / (value + 1))\n",
    "    \n",
    "    # 计算查询文本与文本列表中某个文本的tf-idf相似值\n",
    "    def get_score(self, index, query):\n",
    "        score = 0.0\n",
    "        # 遍历查询文本的每个词\n",
    "        for q in query:\n",
    "            # 判断词是否在文本中，如果没有就继续查下一个\n",
    "            if q not in self.tf[index]:\n",
    "                continue\n",
    "            # 如果有，就将该词的tf-idf值加到score中\n",
    "            score += self.tf[index][q] * self.idf[q]\n",
    "        # 返回最终的得分\n",
    "        return score\n",
    "    \n",
    "    # 计算查询文本与文本列表中每个文本的tf-idf相似值\n",
    "    def get_documents_score(self, query):\n",
    "        score_list = []\n",
    "        for i in range(self.documents_number):\n",
    "            score_list.append(self.get_score(i, query))\n",
    "        return score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30237df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'行政': 0.15384615384615385, '机关': 0.07692307692307693, '强行': 0.07692307692307693, '解除': 0.07692307692307693, '协议': 0.07692307692307693, '造成': 0.07692307692307693, '损失': 0.07692307692307693, '，': 0.07692307692307693, '如何': 0.07692307692307693, '索取': 0.07692307692307693, '赔偿': 0.07692307692307693, '？': 0.07692307692307693}, {'借钱': 0.06666666666666667, '给': 0.06666666666666667, '朋友': 0.06666666666666667, '到期': 0.06666666666666667, '不': 0.06666666666666667, '还': 0.06666666666666667, '得': 0.06666666666666667, '什么': 0.06666666666666667, '时候': 0.06666666666666667, '可以': 0.06666666666666667, '起诉': 0.13333333333333333, '？': 0.13333333333333333, '怎么': 0.06666666666666667}, {'我': 0.058823529411764705, '在': 0.058823529411764705, '微信': 0.058823529411764705, '上': 0.058823529411764705, '被': 0.11764705882352941, '骗': 0.11764705882352941, '了': 0.058823529411764705, '，': 0.058823529411764705, '请问': 0.058823529411764705, '多少': 0.058823529411764705, '钱': 0.058823529411764705, '才': 0.058823529411764705, '可以': 0.058823529411764705, '立案': 0.058823529411764705, '？': 0.058823529411764705}, {'公民': 0.047619047619047616, '对于': 0.047619047619047616, '选举': 0.047619047619047616, '委员会': 0.047619047619047616, '对': 0.047619047619047616, '选民': 0.047619047619047616, '的': 0.09523809523809523, '资格': 0.047619047619047616, '申诉': 0.047619047619047616, '处理': 0.047619047619047616, '决定': 0.047619047619047616, '不服': 0.047619047619047616, '，': 0.047619047619047616, '能': 0.047619047619047616, '不能': 0.047619047619047616, '去': 0.047619047619047616, '法院': 0.047619047619047616, '起诉': 0.047619047619047616, '吗': 0.047619047619047616, '？': 0.047619047619047616}, {'有人': 0.125, '走私': 0.125, '两万元': 0.125, '，': 0.125, '怎么': 0.125, '处置': 0.125, '他': 0.125, '？': 0.125}, {'法律': 0.05, '上': 0.05, '餐具': 0.1, '、': 0.1, '饮具': 0.1, '集中': 0.05, '消毒': 0.1, '服务': 0.05, '单位': 0.05, '的': 0.05, '责任': 0.05, '是不是': 0.05, '对': 0.05, '进行': 0.05, '检验': 0.05, '？': 0.05}]\n",
      "{'行政': 1.0986122886681098, '机关': 1.0986122886681098, '强行': 1.0986122886681098, '解除': 1.0986122886681098, '协议': 1.0986122886681098, '造成': 1.0986122886681098, '损失': 1.0986122886681098, '，': 0.1823215567939546, '如何': 1.0986122886681098, '索取': 1.0986122886681098, '赔偿': 1.0986122886681098, '？': -0.15415067982725836, '借钱': 1.0986122886681098, '给': 1.0986122886681098, '朋友': 1.0986122886681098, '到期': 1.0986122886681098, '不': 1.0986122886681098, '还': 1.0986122886681098, '得': 1.0986122886681098, '什么': 1.0986122886681098, '时候': 1.0986122886681098, '可以': 0.6931471805599453, '起诉': 0.6931471805599453, '怎么': 0.6931471805599453, '我': 1.0986122886681098, '在': 1.0986122886681098, '微信': 1.0986122886681098, '上': 0.6931471805599453, '被': 1.0986122886681098, '骗': 1.0986122886681098, '了': 1.0986122886681098, '请问': 1.0986122886681098, '多少': 1.0986122886681098, '钱': 1.0986122886681098, '才': 1.0986122886681098, '立案': 1.0986122886681098, '公民': 1.0986122886681098, '对于': 1.0986122886681098, '选举': 1.0986122886681098, '委员会': 1.0986122886681098, '对': 0.6931471805599453, '选民': 1.0986122886681098, '的': 0.6931471805599453, '资格': 1.0986122886681098, '申诉': 1.0986122886681098, '处理': 1.0986122886681098, '决定': 1.0986122886681098, '不服': 1.0986122886681098, '能': 1.0986122886681098, '不能': 1.0986122886681098, '去': 1.0986122886681098, '法院': 1.0986122886681098, '吗': 1.0986122886681098, '有人': 1.0986122886681098, '走私': 1.0986122886681098, '两万元': 1.0986122886681098, '处置': 1.0986122886681098, '他': 1.0986122886681098, '法律': 1.0986122886681098, '餐具': 1.0986122886681098, '、': 1.0986122886681098, '饮具': 1.0986122886681098, '集中': 1.0986122886681098, '消毒': 1.0986122886681098, '服务': 1.0986122886681098, '单位': 1.0986122886681098, '责任': 1.0986122886681098, '是不是': 1.0986122886681098, '进行': 1.0986122886681098, '检验': 1.0986122886681098}\n",
      "[0.0021669905358997106, 0.0256563880603619, 0.17167897852134476, 0.0013414703317474394, 0.3648178293578576, 0.08188043947003984]\n"
     ]
    }
   ],
   "source": [
    "document_list = [\"行政机关强行解除行政协议造成损失，如何索取赔偿？\",\n",
    "                 \"借钱给朋友到期不还得什么时候可以起诉？怎么起诉？\",\n",
    "                 \"我在微信上被骗了，请问被骗多少钱才可以立案？\",\n",
    "                 \"公民对于选举委员会对选民的资格申诉的处理决定不服，能不能去法院起诉吗？\",\n",
    "                 \"有人走私两万元，怎么处置他？\",\n",
    "                 \"法律上餐具、饮具集中消毒服务单位的责任是不是对消毒餐具、饮具进行检验？\"]\n",
    "\n",
    "import jieba\n",
    "document_list = [list(jieba.cut(doc)) for doc in document_list]\n",
    "\n",
    "tf_idf_model = TF_IDF_Model(document_list)\n",
    "\n",
    "print(tf_idf_model.tf)\n",
    "print(tf_idf_model.idf)\n",
    "\n",
    "query = \"走私了两万元，在法律上应该怎么量刑？\"\n",
    "query = list(jieba.cut(query))\n",
    "scores = tf_idf_model.get_documents_score(query)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4c8d51",
   "metadata": {},
   "source": [
    "# TF_IDF关键词提取（基于jieba）\n",
    "\n",
    "\n",
    "- import jieba.analyse\n",
    "\n",
    "* jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())\n",
    "    * sentence 为待提取的文本\n",
    "    * topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20\n",
    "    * withWeight 为是否一并返回关键词权重值，默认值为 False\n",
    "    * allowPOS 仅包括指定词性的词，默认值为空，即不筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf530c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "韦少  杜兰特  全明星  全明星赛  MVP  威少  正赛  科尔  投篮  勇士  球员  斯布鲁克  更衣柜  NBA  三连庄  张卫平  西部  指导  雷霆  明星队\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "\n",
    "with open('NBA.txt', encoding='utf-8') as f:\n",
    "    lines = f.read()\n",
    "print(\"  \".join(analyse.extract_tags(lines, topK=20, withWeight=False, allowPOS=())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91425d7f",
   "metadata": {},
   "source": [
    "# TextRank关键词提取（基于jieba）\n",
    "\n",
    "* jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 直接使用，接口相同，注意默认过滤词性。\n",
    "* jieba.analyse.TextRank() 新建自定义 TextRank 实例\n",
    "\n",
    "* 算法论文： [TextRank: Bringing Order into Texts](http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)\n",
    "\n",
    "* 基本思想:\n",
    "    * 将待抽取关键词的文本进行分词\n",
    "    * 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图\n",
    "    * 计算图中节点的PageRank，注意是无向带权图\n",
    "* 阅读资料：\n",
    "    * [基于textrank的关键词抽取方法](https://blog.csdn.net/zhangf666/article/details/77841845)\n",
    "    * [pagerank算法核心思想](https://www.jianshu.com/p/f6d66ab97332)\n",
    "    * [浅析PageRank算法](http://blog.codinglabs.org/articles/intro-to-pagerank.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e977b49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全明星赛  勇士  正赛  指导  对方  投篮  球员  没有  出现  时间  威少  认为  看来  结果  相隔  助攻  现场  三连庄  介绍  嘉宾\n",
      "---------------------分割线----------------\n",
      "勇士  正赛  全明星赛  指导  投篮  玩命  时间  对方  现场  结果  球员  嘉宾  时候  全队  主持人  照片  全程  目标  快船队  肥皂剧\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "\n",
    "with open('NBA.txt', encoding='utf-8') as f:\n",
    "    lines = f.read()\n",
    "print(\"  \".join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))))\n",
    "print(\"---------------------分割线----------------\")\n",
    "print(\"  \".join(analyse.textrank(lines, topK=20, withWeight=False, allowPOS=('ns', 'n'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4732c5af",
   "metadata": {},
   "source": [
    "# glove加载并表示词向量（使用gensim加载）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d5e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 输入文件\n",
    "glove_file = './glove.6B.100d.txt'\n",
    "# 输出文件\n",
    "tmp_file = \"glove2word2vec.txt\"\n",
    "\n",
    "# 转换\n",
    "(count, dimensions) = glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "print(count)\n",
    "print(dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3cfa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载转换后的文件\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a49aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取某个单词的向量表示，直接以下标方式访问即可\n",
    "cat_vec = model['cat']\n",
    "print(cat_vec)\n",
    "# 获得单词cat的最相似向量的词汇\n",
    "print(model.most_similar('cat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455633bc",
   "metadata": {},
   "source": [
    "# fasttext生成词向量（使用gensim）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97b40dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我 爱 我 的 祖国', '我 喜欢 祖国 的 大好河山']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "print(samples)\n",
    "fasttext_model = FastText(samples,  vector_size=20, window=3, min_count=1, epochs=10, min_n=3 , max_n=6,word_ngrams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd0c33bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02302497  0.00666335  0.03425618 -0.00233102  0.03433145 -0.01923007\n",
      " -0.01505522  0.02630648 -0.03774588 -0.01843461 -0.00756334 -0.00232549\n",
      " -0.00049289 -0.02973611  0.00931649 -0.00748862  0.01399848 -0.03564914\n",
      " -0.0082388  -0.03114243]\n"
     ]
    }
   ],
   "source": [
    "print(fasttext_model.wv['我'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee161d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
